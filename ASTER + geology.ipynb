{
 "metadata": {
  "name": "",
  "signature": "sha256:407f684e53bdc16e122fea3a2ee4eb142c7967975faeec388fede0631245f6b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "## Subsetting ASTER using geological data\n",
      "\n",
      "This is the code used to generate the ASTER subsetting example used in Jess' talk in the Roadshow"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "### Getting ASTER data\n",
      "\n",
      "There's a seperate WCS endpoint for each ASTER product. We're going to grab a few of the compositional products. Here I've pulled the WCS URL from the AuScope portal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Products to pull\n",
      "aster_products = [\n",
      "    'AlOH_group_composition',\n",
      "    'Ferric_oxide_composition',\n",
      "    'MgOH_group_composition'\n",
      "]\n",
      "bounds = (119.52, -21.6, 120.90, -20.5)\n",
      "wcsurl = 'http://aster.nci.org.au/thredds/wcs/aster/vnir/Aus_Mainland/Aus_Mainland_{0}_reprojected.nc4'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Then we can hit up the webservices for some data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib import wcs\n",
      "import socket\n",
      "\n",
      "# # Only uncomment this if you're not giving a talk and can afford to wait for the ~18 Mb download\n",
      "# for product in aster_products:\n",
      "#     try:\n",
      "#         coverage_service = wcs.WebCoverageService(wcsurl.format(product))\n",
      "#         response = coverage_service.getCoverage(product,\n",
      "#                                                 bbox=bounds,       # We only want data within the bounds we're considering\n",
      "#                                                 format='GeoTIFF',  # We want data in GeoTIFF format\n",
      "#                                                 timeout=100)       # We need to wait a bit longer for large files...\n",
      "#         with open(product + '.gtif', 'wb') as fhandle:\n",
      "#             fhandle.write(response.read())\n",
      "#     except socket.timeout:\n",
      "#         continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "If you're sick of waiting, you can just copy the data out of the geotiffs folder"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!!cp geotiffs/*.geotiff ."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Now we can fire up rasterio and have a play with the ASTER coverage data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import rasterio, os\n",
      "\n",
      "# Just look at the first image we have\n",
      "filename = os.path.join('geotiffs', aster_products[0] + '.geotiff')\n",
      "\n",
      "# Use rasterio to load the geotiff\n",
      "with rasterio.drivers():\n",
      "    with rasterio.open(filename) as src:\n",
      "        image = src.read()[0]  # We only have one band here\n",
      "        \n",
      "        # Print some of the information about the geotiff\n",
      "        print src.crs_wkt, '\\n'\n",
      "        print src.bounds, '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "...and we can plot this data as an image"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import normalize  # handles the masked points in the data\n",
      "\n",
      "fig = figure(figsize=(22, 11))\n",
      "grid = GridSpec(1, 3)\n",
      "with rasterio.drivers():\n",
      "    for gs, product in zip(grid, aster_products):\n",
      "        filename = os.path.join('geotiffs', product + '.geotiff')\n",
      "        with rasterio.open(filename) as src:\n",
      "            image = src.read()[0]  # We only have one band here\n",
      "\n",
      "            # Plot the ASTER data\n",
      "            ax = subplot(gs)\n",
      "            ax.imshow(normalize(image), cmap=get_cmap('spectral'))\n",
      "            ax.set_title(product)\n",
      "            ax.set_axis_off()\n",
      "\n",
      "fig.savefig('graphics/aster_example.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "### Getting geological data\n",
      "\n",
      "We're going to hit the endpoints for the WA 1:1M geological map. GA in their wisdom has split the data into a seperate WFS for contacts, faults and geologic units, but the WFS url follows a common template:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# West Australian geology\n",
      "wfsurl_template = (\"http://www.ga.gov.au/geows/{0}/oneg_wa_1m/wfs\")\n",
      "geologic_objects = ('contacts', 'faults', 'geologicunits')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "If you want to look at other datasets from GA, they have a nice list here: http://www.ga.gov.au/data-pubs/web-services/ga-web-services\n",
      "\n",
      "So now we actually need to get the data from the webservices. We run into a data size problem pretty quickly, where the server timeouts because it takes to long to get the data if the bounding box is massive. pySISS provides a way around this using the 'post_block_requests' function, which subdivieds our bounding box into a number of smaller blocks and submits these as seperate requests so that the return times are faster.\n",
      "\n",
      "The subrequests are dumped into a folder called 'tmp' for us to use later"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pysiss.webservices import post_block_requests\n",
      "import os\n",
      "\n",
      "# # Again, only uncomment if you're not running a talk and don't mind\n",
      "# # waiting for a few minutes for these to download\n",
      "# for obj in geologic_objects:\n",
      "#     post_block_requests(wfsurl=wfsurl_template.format(obj), \n",
      "#                         filename=obj,\n",
      "#                         lower_corner=(119.52, -20.5),\n",
      "#                         upper_corner=(120.90, -21.6),\n",
      "#                         max_features=500,\n",
      "#                         nx_blocks=3)\n",
      "\n",
      "os.listdir('tmp')[:10]  # List first 10 files in tmp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Since we know that everything returned from these WFSes is a gsml:MappedFeature instance, we can iterate over each of these using pySISS's unmarshal_all function, which interprets all the elements of a particular type in the file. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pysiss.vocabulary.unmarshal import unmarshal_all\n",
      "from collections import defaultdict\n",
      "\n",
      "# We only want to capture each feature once (it may occur more than once\n",
      "# across different subblocks) so we'll stash the identifiers for each feature\n",
      "idents = set()\n",
      "features = defaultdict(list)\n",
      "\n",
      "# Get the resulting mapped features\n",
      "os.chdir('tmp')\n",
      "for filename in os.listdir('.'):\n",
      "    # Pull out all the mapped features\n",
      "    mapped_features = unmarshal_all(filename, \n",
      "                                    tag='gsml:MappedFeature')\n",
      "    \n",
      "    # Stash the features if we don't already have them\n",
      "    for feature in mapped_features:\n",
      "        if feature.ident not in idents:\n",
      "            features[feature.type].append(feature)\n",
      "            idents.add(feature.ident)\n",
      "os.chdir('..')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Now we've got a nice dictionary of all the features keyed by feature type"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contacts = features['gsml:Contact']\n",
      "faults = features['gsml:Fault']\n",
      "geologic_units = features['gsml:GeologicUnit']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "pySISS returns a fairly minimal object which contains (a) a unique identifier (actually it just uses the same one assigned by GA in this instance), (b) some kind of geometry data (given as a Shapely object), (c) a type attribute, which gives the type of the feature (e.g. gsml:Fault, gsml:geologicUnit etc), and a metadata instance, which is an XML etree which can be queried using xpath."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's take a look at the first geologic unit\n",
      "unit = geologic_units[0]\n",
      "\n",
      "print unit.ident"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print unit.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from plotting import plot_feature_shape\n",
      "\n",
      "plot_feature_shape(unit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print unit.metadata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "print etree.tostring(unit.metadata.tree, pretty_print=True)[:1497]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "We can query the metadata to get the age of the unit, for example. This also shows that pySISS is aware of most of the GSML/GML/XML namespaces and is able to convert between short and long versions using a NamespaceRegistry, and the functions shorten_namespace and expand_namespace."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pysiss.vocabulary.namespaces import NamespaceRegistry, shorten_namespace\n",
      "\n",
      "def get_age(feature):\n",
      "    \"\"\" Returns the gsml:preferredAge of the given feature, or\n",
      "        None if the feature doesn't have this metadata\n",
      "    \"\"\"\n",
      "    age = feature.metadata.xpath('.//gsml:preferredAge//gsml:value',\n",
      "                              namespaces=NamespaceRegistry())\n",
      "    if age:\n",
      "        return age[0].text\n",
      "    else:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "age = get_age(unit)\n",
      "\n",
      "print 'Full namespace: ' + age\n",
      "print 'Short namespace: ' + shorten_namespace(age)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "We can also get all the unique values for a given piece of metadata"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages = set()\n",
      "for feature in geologic_units:\n",
      "    age_elem = feature.metadata.find(\n",
      "        './/gsml:preferredAge//gsml:value', \n",
      "        namespaces=NamespaceRegistry())\n",
      "    ages.add(shorten_namespace(age_elem.text))\n",
      "ages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "pySISS can't currently sort by age nicely but it's on the to do list..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "### Styling based on geological metadata\n",
      "\n",
      "We can also query the lithologies of the units that we know about using XPath queries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_lithology(feature):\n",
      "    \"\"\" Get the lithology from a feature, if it exists.\n",
      "    \"\"\"\n",
      "    lithology = feature.metadata.xpath(\n",
      "        './/gsml:RockMaterial/gsml:lithology/@xlink:href', \n",
      "        namespaces=NamespaceRegistry())\n",
      "    if lithology:\n",
      "        return lithology[0]\n",
      "    else:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Find all the unique lithologies that we have..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_lithologies = set()\n",
      "for feature in geologic_units:\n",
      "    lith = get_lithology(feature)\n",
      "    unique_lithologies.add(shorten_namespace(lith))\n",
      "    \n",
      "unique_lithologies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Ok, so we can pull out metadata, which lets us plot the units according to their lithology. I've made a style file, which keys colors to a particular metadata value. This should be sitting in the same directory as this notebook..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!!head lithology_styles.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "OK! Now we're ready to plot things..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from descartes import PolygonPatch\n",
      "from plotting import style_by_lithology, plot_contacts, plot_faults\n",
      "\n",
      "fig = figure(figsize=(22, 22))\n",
      "axes = gca()\n",
      "\n",
      "for feature in geologic_units:\n",
      "    axes.add_patch(\n",
      "        PolygonPatch(feature.shape,\n",
      "                     alpha=0.8, zorder=0, \n",
      "                     **style_by_lithology(feature)))\n",
      "\n",
      "plot_contacts(axes, contacts)\n",
      "plot_faults(axes, faults)\n",
      "axes.set_axis_off()\n",
      "fig.savefig('graphics/geo_map_lith.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "...or styled by age:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from plotting import style_by_age\n",
      "\n",
      "fig = figure(figsize=(22, 22))\n",
      "axes = gca()\n",
      "\n",
      "for feature in geologic_units:\n",
      "    axes.add_patch(\n",
      "        PolygonPatch(feature.shape,\n",
      "                    alpha=0.8, zorder=0, \n",
      "                     **style_by_age(feature)))\n",
      "\n",
      "plot_contacts(axes, contacts)\n",
      "plot_faults(axes, faults)\n",
      "axes.set_axis_off()\n",
      "fig.savefig('graphics/geo_map_ages.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "## Pulling out particular units\n",
      "\n",
      "So the next thing to do is pull out the units that we're interested in:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Quick function to return a shortened lithology key\n",
      "short_lithology = lambda x: shorten_namespace(get_lithology(x)) \n",
      "\n",
      "# Pick the lithologies we're interested in\n",
      "lithologies_to_get = set((\n",
      "    'cgi_slith:basaltic_rock',\n",
      "    'cgi_slith:doleritic_rock',\n",
      "    'cgi_slith:gabbro',\n",
      "    'cgi_slith:syenitic_rock',\n",
      "    'cgi_slith:serpentinite'\n",
      "))\n",
      "\n",
      "# Filter all our units\n",
      "units = [u for u in geologic_units\n",
      "         if short_lithology(u) in lithologies_to_get]\n",
      "idents = [u.ident for u in units]\n",
      "idents[:5]  # Print the first 5 we get"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "We can also map the units we've picked in red"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = figure(figsize=(11, 11))\n",
      "axes = gca()\n",
      "for feature in geologic_units:\n",
      "    params = dict(facecolor='gray', edgecolor='gray')\n",
      "    if feature.ident in idents:\n",
      "        params['facecolor'] ='red'\n",
      "    axes.add_patch(\n",
      "        PolygonPatch(feature.shape,\n",
      "                     alpha=0.8, zorder=0, \n",
      "                     **params))\n",
      "\n",
      "axes.set_xlim(119.52, 120.90)\n",
      "axes.set_ylim(-21.6, -20.5)\n",
      "axes.set_aspect('equal')\n",
      "axes.set_axis_off()\n",
      "fig.savefig('graphics/geo_map_selection.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "To speed things up a little, we're going to merge the polygons that touch each other into a single polygon, and then stash the geometries in a shapefile so that can it be used by gdal"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shapely.ops import cascaded_union\n",
      "from shapely.geometry import mapping\n",
      "import fiona\n",
      "\n",
      "polygons = cascaded_union([u.shape for u in units])\n",
      "\n",
      "# Define a polygon feature geometry with one attribute\n",
      "schema = {\n",
      "    'geometry': 'MultiPolygon',\n",
      "    'properties': {'id': 'int'},\n",
      "}\n",
      "\n",
      "# Write a new Shapefile\n",
      "with fiona.open('selected_units.shp', 'w', 'ESRI Shapefile', schema) as c:\n",
      "    c.write({\n",
      "        'geometry': mapping(polygons),\n",
      "        'properties': {'id': 123},\n",
      "    })"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "...and then we're going to rasterize the geometry. Rasterio's rasterize method is really flaky here so we'll fall back to using the raw gdal bindings..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import rasterio\n",
      "from osgeo import ogr, gdal\n",
      "\n",
      "geotiff = gdal.GetDriverByName('GTiff')\n",
      "\n",
      "aster_products = [\n",
      "    'AlOH_group_composition',\n",
      "    'Ferric_oxide_composition',\n",
      "    'MgOH_group_composition'\n",
      "]\n",
      "bounds = (119.52, -21.6, 120.90, -20.5)\n",
      "mask_value = 1\n",
      "masked_images = {}\n",
      "\n",
      "with rasterio.drivers():\n",
      "    for product in aster_products:\n",
      "        with rasterio.open(product + '.geotiff') as src:\n",
      "            # Make a new tiff file to store the rasterized mask\n",
      "            target_filename = 'output.geotiff'\n",
      "            target_ds = geotiff.Create(target_filename, src.shape[1], src.shape[0], 1, gdal.GDT_Byte)\n",
      "            target_ds.SetGeoTransform(src.transform)\n",
      "            band = target_ds.GetRasterBand(1)\n",
      "            band.SetNoDataValue(-1)\n",
      "            \n",
      "            # Load the shapefile\n",
      "            source_ds = ogr.Open('selected_units.shp')\n",
      "            source_layer = source_ds.GetLayer()\n",
      "            \n",
      "            # Rasterize the vector data to make a mask\n",
      "            gdal.RasterizeLayer(target_ds, [1], source_layer, burn_values=[1])\n",
      "            mask = target_ds.ReadAsArray()\n",
      "            \n",
      "            # Use our mask to mask out the values in our ASTER data\n",
      "            image = normalize(src.read_band(1))\n",
      "            masked_images[product] = \\\n",
      "                numpy.ma.MaskedArray(image.data, \n",
      "                                     mask=numpy.logical_or(\n",
      "                                        image.mask,\n",
      "                                        numpy.logical_not(mask)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for product, image in masked_images.items():\n",
      "    fig = figure(figsize=(11, 11))\n",
      "    axes = gca()\n",
      "    axes.imshow(image)\n",
      "    axes.set_axis_off()\n",
      "    axes.set_aspect('equal')\n",
      "    fig.savefig('graphics/' + product + '_filter.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "The next thing is to turn each of our arrays into a vector of points for analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Only keep points for which we have values in all three layers\n",
      "uber_mask = reduce(numpy.logical_and, [~im.mask for im in masked_images.values()])\n",
      "vectors = numpy.asarray([im[uber_mask] for im in masked_images.values()])\n",
      "vectors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Now we can use our data for whatever we want. Here I'm just going to run it through a PCA on 10% of the dataset and then reproject the data back onto the maps to get maps of the biggest compositional variations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "X_r = pca.fit(vectors).transform(vectors)\n",
      "\n",
      "# Percentage of variance explained for each components\n",
      "print 'explained variance ratio (first two components): {0}'.format(pca.explained_variance_ratio_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "Having done our PCA, we can map it back to the original space"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_vector = numpy.zeros(shape=masked_images.values()[0].shape)\n",
      "pca_vector[uber_mask] = pca.components_[0]\n",
      "pca_vector = numpy.ma.MaskedArray(pca_vector, mask=numpy.logical_not(uber_mask))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "outputs": [],
     "source": [
      "and we're going to use scikits image to blow up the individual pixels a bit so we can see them..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import ndimage\n",
      "\n",
      "# Set up dilation for the mask\n",
      "struct = ndimage.generate_binary_structure(2, 8)\n",
      "double_dilate = lambda x: \\\n",
      "    ndimage.binary_dilation(\n",
      "        ndimage.binary_dilation(\n",
      "            ndimage.binary_dilation(x, structure=struct), \n",
      "            structure=struct), \n",
      "        structure=struct)\n",
      "\n",
      "dilated_pca_vector = numpy.ma.MaskedArray(\n",
      "    ndimage.grey_dilation(pca_vector, size=(15,15)),\n",
      "    mask=~double_dilate(~pca_vector.mask))\n",
      "\n",
      "fig = figure(figsize=(22, 22))\n",
      "axes = gca()\n",
      "axes.imshow(dilated_pca_vector)\n",
      "axes.set_axis_off()\n",
      "fig.savefig('graphics/PCA1.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = figure(figsize=(22, 22))\n",
      "axes = gca()\n",
      "axes.imshow(dilated_pca_vector[1300:2500, 500:2000])\n",
      "axes.set_axis_off()\n",
      "fig.savefig('graphics/PCA1_zoom.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}